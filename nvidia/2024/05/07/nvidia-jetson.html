<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>How to install SLE-15-SP6 on NVIDIA’s Jetson AGX Orin, Jetson Orin Nano/NX and IGX Orin | Stefan’s openSUSE Blog</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="How to install SLE-15-SP6 on NVIDIA’s Jetson AGX Orin, Jetson Orin Nano/NX and IGX Orin" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This covers the installation of updated Kernel, out-of-tree nvidia kernel modules package, how to get GNOME desktop running and installation/run of glmark2 benchmark. Also it describes how to get some CUDA and TensorRT samples running. In addition it describes the firmware update on Jetson AGX Orin and Jetson Orin Nano and how to connect a serial console to Jetson Orin Nano." />
<meta property="og:description" content="This covers the installation of updated Kernel, out-of-tree nvidia kernel modules package, how to get GNOME desktop running and installation/run of glmark2 benchmark. Also it describes how to get some CUDA and TensorRT samples running. In addition it describes the firmware update on Jetson AGX Orin and Jetson Orin Nano and how to connect a serial console to Jetson Orin Nano." />
<link rel="canonical" href="/nvidia/2024/05/07/nvidia-jetson.html" />
<meta property="og:url" content="/nvidia/2024/05/07/nvidia-jetson.html" />
<meta property="og:site_name" content="Stefan’s openSUSE Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-05-07T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="How to install SLE-15-SP6 on NVIDIA’s Jetson AGX Orin, Jetson Orin Nano/NX and IGX Orin" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-03-11T19:11:38+00:00","datePublished":"2024-05-07T00:00:00+00:00","description":"This covers the installation of updated Kernel, out-of-tree nvidia kernel modules package, how to get GNOME desktop running and installation/run of glmark2 benchmark. Also it describes how to get some CUDA and TensorRT samples running. In addition it describes the firmware update on Jetson AGX Orin and Jetson Orin Nano and how to connect a serial console to Jetson Orin Nano.","headline":"How to install SLE-15-SP6 on NVIDIA’s Jetson AGX Orin, Jetson Orin Nano/NX and IGX Orin","mainEntityOfPage":{"@type":"WebPage","@id":"/nvidia/2024/05/07/nvidia-jetson.html"},"url":"/nvidia/2024/05/07/nvidia-jetson.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Stefan&apos;s openSUSE Blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Stefan&#39;s openSUSE Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">How to install SLE-15-SP6 on NVIDIA&#39;s Jetson AGX Orin, Jetson Orin Nano/NX and IGX Orin</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-05-07T00:00:00+00:00" itemprop="datePublished">May 7, 2024
      </time>• <span class="dt-modified">Last updated on Mar 11, 2025</span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>This covers the installation of updated Kernel, out-of-tree nvidia kernel modules package, how to get GNOME desktop running and installation/run of glmark2 benchmark. Also it describes how to get some CUDA and TensorRT samples running. In addition it describes the firmware update on <code class="language-plaintext highlighter-rouge">Jetson AGX Orin</code> and <code class="language-plaintext highlighter-rouge">Jetson Orin Nano</code> and how to connect a serial console to <code class="language-plaintext highlighter-rouge">Jetson Orin Nano</code>.</p>

<h3 id="firmware-update-on-jetson-agx-orin">Firmware Update on Jetson AGX Orin</h3>

<p>On <code class="language-plaintext highlighter-rouge">Jetson AGX Orin</code> first update the firmware to Jetpack 6.1/36.4.0.</p>

<p>Download <a href="https://developer.nvidia.com/downloads/embedded/l4t/r36_release_v4.0/release/Jetson_Linux_R36.4.0_aarch64.tbz2">Driver Package (BSP)</a> from this <a href="https://developer.nvidia.com/embedded/jetson-linux-r3640">location</a>. Extract <code class="language-plaintext highlighter-rouge">Jetson_Linux_R36.4.0_aarch64.tbz2</code>.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">tar </span>xf Jetson_Linux_R36.4.0_aarch64.tbz2</code></pre></figure>

<p>Then connect with two cables your computer to the Micro-USB port and Type-C port (next to the 40pin connector) of <code class="language-plaintext highlighter-rouge">Jetson AGX Orin</code>.
Now switch <code class="language-plaintext highlighter-rouge">Jetson AGX Orin</code> to recovery mode (using your Micro-USB cable).</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">cd </span>Linux_for_Tegra
<span class="nb">sudo</span> ./tools/board_automation/boardctl <span class="nt">-t</span> topo recovery</code></pre></figure>

<p>Check that <code class="language-plaintext highlighter-rouge">Jetson AGX Orin</code> is now in recovery mode.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">lsusb
<span class="o">[</span>...]
Bus 003 Device 099: ID 0955:7023 NVIDIA Corp. APX
<span class="o">[</span>...]</code></pre></figure>

<p>Now flash your firmware (using the Type-C cable). Make sure you have package <code class="language-plaintext highlighter-rouge">dtc</code> installed, because the tool <code class="language-plaintext highlighter-rouge">fdtoverlay</code> is needed.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo</span> ./flash.sh p3737-0000-p3701-0000-qspi external</code></pre></figure>

<p>Reboot <code class="language-plaintext highlighter-rouge">Jetson AGX Orin</code>.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo</span> ./tools/board_automation/boardctl <span class="nt">-t</span> topo power_on</code></pre></figure>

<p>After reboot you should see in the Firmware setup - shown on your monitor or on your serial console - the firmware version <code class="language-plaintext highlighter-rouge">36.4.0-gcid-XXXXXXXX</code>.</p>

<h3 id="firmware-update-on-jetson-orin-nano">Firmware Update on Jetson Orin Nano</h3>

<p>Updating the firmware on <code class="language-plaintext highlighter-rouge">Jetson Orin Nano</code> is similar to the process above for <code class="language-plaintext highlighter-rouge">Jetson AGX Orin</code>.</p>

<p>Unfortunately the board automation tools do not support <code class="language-plaintext highlighter-rouge">Jetson Orin Nano</code>. Therefore for switching this device in recovery mode instead of running <code class="language-plaintext highlighter-rouge">boardctl</code> you need to connect two pins or put a jumper on both respectively. These are the pins 9/10 (GND/FC REC) of the 12-pin <code class="language-plaintext highlighter-rouge">J14</code> “button” header of carrier board located under the Jetson module (right below the fan next to the SD card slot).</p>

<p>So disconnect <code class="language-plaintext highlighter-rouge">Jetson AGX Orin</code> from power, then connect these pins and then reconnect power. With that the device should be in Recovery mode. Connect an USB cable to the Type-C port of <code class="language-plaintext highlighter-rouge">Jetson AGX Orin</code> and check if it is now in Recovery mode.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">lsusb
<span class="o">[</span>...]
Bus 003 Device 105: ID 0955:7523 NVIDIA Corp. APX
<span class="o">[</span>...]</code></pre></figure>

<p>Now flash your firmware. Make sure you have package <code class="language-plaintext highlighter-rouge">dtc</code> installed, because the tool <code class="language-plaintext highlighter-rouge">fdtoverlay</code> is needed.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo</span> ./flash.sh p3768-0000-p3767-0000-a0-qspi external</code></pre></figure>

<p>Disconnect <code class="language-plaintext highlighter-rouge">Jetson AGX Orin</code> from power and reconnect it to power. After reboot you should see in the Firmware setup - shown on your monitor or on your serial console - the firmware version <code class="language-plaintext highlighter-rouge">36.4.0-gcid-XXXXXXXX</code>.</p>

<h3 id="serial-console-on-jetson-orin-nano">Serial Console on Jetson Orin Nano</h3>

<p>In order to have a serial console on <code class="language-plaintext highlighter-rouge">Jetson Orin Nano</code> you need a 3.3.V USB-UART adapter/cable. Connect it to the pins 3/4/7 (RXD/TXD/GrouND) of the 12-pin <code class="language-plaintext highlighter-rouge">J14</code> “button” header of carrier board located under the Jetson module (right below the fan next to the SD card slot).</p>

<h3 id="sp6">SP6</h3>

<p>Download <a href="https://www.suse.com/download/sles/">SLE-15-SP6 (Arm) installation image</a>. This you can put on a regular USB stick or on an SD card using <code class="language-plaintext highlighter-rouge">dd</code> command.</p>

<p>Boot from the USB stick/SD card, that you wrote above and install SP6. You can install via serial console or connect a monitor to the display port.</p>

<h4 id="when-using-a-connected-monitor-for-installation">When using a connected monitor for installation</h4>

<p>This needs for the installation a special setting in the Firmware of the machine.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nt">--</span><span class="o">&gt;</span> UEFI Firmware Settings
 <span class="nt">--</span><span class="o">&gt;</span> Device Manager
  <span class="nt">--</span><span class="o">&gt;</span> NVIDIA Configuration
   <span class="nt">--</span><span class="o">&gt;</span> Boot Configuration
    <span class="nt">--</span><span class="o">&gt;</span> SOC Display Hand-Off Mode &lt;Always&gt;</code></pre></figure>

<p>This setting for <code class="language-plaintext highlighter-rouge">SOC Display Hand-Off Mode</code> will change automatically to <code class="language-plaintext highlighter-rouge">Never</code> later with the installation of the graphics driver.</p>

<h4 id="installation">Installation</h4>

<p>Once grub starts you need to edit the grub entry <code class="language-plaintext highlighter-rouge">Installation</code>. Press <code class="language-plaintext highlighter-rouge">e</code> for doing this and add <code class="language-plaintext highlighter-rouge">console=tty0 exec="date -s 2025-01-27"</code> (when using a connected monitor for intallation) or <code class="language-plaintext highlighter-rouge">exec="date -s 2025-01-27"</code> (when installing on a serial console and add also <code class="language-plaintext highlighter-rouge">console=ttyTCU0,115200</code> on <code class="language-plaintext highlighter-rouge">Jetson Orin Nano</code>) to the <code class="language-plaintext highlighter-rouge">linux [...]</code> line. Replace <code class="language-plaintext highlighter-rouge">2025-01-27</code> with the current date.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c">### When using a connected monitor for intallation</span>
<span class="o">[</span>...]
linux /boot/aarch64/linux <span class="nv">splash</span><span class="o">=</span>silent <span class="nv">console</span><span class="o">=</span>tty0 <span class="nb">exec</span><span class="o">=</span><span class="s2">"date -s 2025-01-27"</span>
<span class="o">[</span>...]</code></pre></figure>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c">### When installing on a serial console</span>
<span class="o">[</span>...]
linux /boot/aarch64/linux <span class="nv">splash</span><span class="o">=</span>silent <span class="nb">exec</span><span class="o">=</span><span class="s2">"date -s 2025-01-27"</span>
<span class="c"># On Jetson Orin Nano</span>
linux /boot/aarch64/linux <span class="nv">splash</span><span class="o">=</span>silent <span class="nv">console</span><span class="o">=</span>ttyTCU0,115200 <span class="nb">exec</span><span class="o">=</span><span class="s2">"date -s 2025-01-27"</span>
<span class="o">[</span>...]</code></pre></figure>

<p>The reason for this is that during installation the driver <code class="language-plaintext highlighter-rouge">nvvrs-pseq-rtc</code> for the battery-backed RTC0 (Real Time Clock) is not yet available and therefore the non-battery-backed RTC1 is used, which doesn’t have the correct time set during installation. So this is a workaround to avoid a product registration failure later due to a certificate, which is not valid yet.</p>

<p>Then press <code class="language-plaintext highlighter-rouge">F10</code> to continue to boot.</p>

<p>Make sure you select the following modules during installation:</p>

<ul>
  <li>Basesystem (enough for just installing the kernel driver)</li>
  <li>Containers (needed for podman for CUDA libraries)</li>
  <li>Desktop Applications (needed for running a desktop)</li>
  <li>Development Tools (needed for git for CUDA samples)</li>
</ul>

<p>Select <code class="language-plaintext highlighter-rouge">SLES with GNOME</code> for installation.</p>

<p>In <code class="language-plaintext highlighter-rouge">Clock and Time Zone</code> dialogue chose <code class="language-plaintext highlighter-rouge">Other Settings</code> to open <code class="language-plaintext highlighter-rouge">Change Date and Time</code> dialogue. There enable <code class="language-plaintext highlighter-rouge">Synchronize with NTP Server</code>.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nt">--</span><span class="o">&gt;</span> Clock and Time Zone dialogue
 <span class="nt">--</span><span class="o">&gt;</span> Other Settings
  <span class="nt">--</span><span class="o">&gt;</span> Change Date and Time dialogue
   <span class="nt">--</span><span class="o">&gt;</span> <span class="o">(</span>x<span class="o">)</span> Synchronize with NTP Server</code></pre></figure>

<h3 id="kernel--kmp-drivers">Kernel + KMP drivers</h3>

<p>After installation update kernel and install our KMP (kernel module package) for all nvidia kernel modules.</p>

<h4 id="installation-on-nvidias-jetson-agx-orin-and-jetson-orin-nanonx">Installation on NVIDIA’s Jetson AGX Orin and Jetson Orin Nano/NX</h4>

<p>The KMP is available as a driver kit via the SolidDriver Program. For installation please use the following commands:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># flavor either default or 64kb (check with `uname -r` command)</span>
<span class="nb">sudo </span>zypper up kernel-&lt;flavor&gt;
<span class="nb">sudo </span>zypper ar https://drivers.suse.com/nvidia/Jetson/Jetson_Linux_36.4/sle-15-sp6-aarch64/1.0/install jetson-kmp
<span class="nb">sudo </span>zypper ar https://drivers.suse.com/nvidia/Jetson/Jetson_Linux_36.4/sle-15-sp6-aarch64/1.0/update  jetson-kmp-update
<span class="nb">sudo </span>zypper ref
<span class="nb">sudo </span>zypper <span class="k">in</span> <span class="nt">-r</span> jetson-kmp nvidia-jetson-36_4-kmp-&lt;flavor&gt;</code></pre></figure>

<h4 id="installation-on-nvidia-igx-orin">Installation on NVIDIA IGX Orin</h4>

<p>We plan to make the KMP available as a driver kit via the SolidDriver Program. For now please install an updated kernel and the KMP after checking the <a href="https://build.opensuse.org/project/monitor/X11:XOrg">build status</a> (type ‘igx’ in Search… field; rebuilding can take a few hours!) from our open buildservice:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># flavor either default or 64kb (check with `uname -r` command)</span>
<span class="nb">sudo </span>zypper up kernel-&lt;flavor&gt;
<span class="nb">sudo </span>zypper ar https://download.opensuse.org/repositories/X11:/XOrg/SLE_15_SP6/ igx-kmp
<span class="nb">sudo </span>zypper ref
<span class="nb">sudo </span>zypper <span class="k">in</span> <span class="nt">-r</span> jetson-kmp nvidia-igx-kmp-&lt;flavor&gt;</code></pre></figure>

<h3 id="userspacedesktop">Userspace/Desktop</h3>

<h4 id="installation-on-nvidias-jetson-agx-orin-and-jetson-orin-nanonx-1">Installation on NVIDIA’s Jetson AGX Orin and Jetson Orin Nano/NX</h4>

<p>Please install userspace on these devices by using the following commands:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>zypper ar https://repo.download.nvidia.com/jetson/sle15-sp6/jp6.1/ jetson-userspace 
<span class="nb">sudo </span>zypper ref 
<span class="nb">sudo </span>zypper <span class="k">in </span>nvidia-jetpack-all</code></pre></figure>

<h4 id="installation-on-nvidia-igx-orin-1">Installation on NVIDIA IGX Orin</h4>

<p>Unfortunately installing the userspace on this device is still a non-trivial task.</p>

<p>Download <a href="https://developer.nvidia.com/downloads/igx/v1.1/Jetson_Linux_R36.4.1_aarch64.tbz2">Bootloader(QSPI) Package</a> from this <a href="https://developer.nvidia.com/igx-downloads">location</a> (select <code class="language-plaintext highlighter-rouge">IGX-SW 1.1 Production Release</code>). Extract <code class="language-plaintext highlighter-rouge">Jetson_Linux_R36.4.1_aarch64.tbz2</code>.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">tar </span>xf Jetson_Linux_R36.4.1_aarch64.tbz2</code></pre></figure>

<p>Then you need to convert debian packages from this content into tarballs.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">pushd </span>Linux_for_Tegra
<span class="nb">sed</span> <span class="nt">-i</span> <span class="nt">-e</span> <span class="s1">'s/lbzip2/bzip2/g'</span> <span class="nt">-e</span> <span class="s1">'s/-I zstd //g'</span> nv_tools/scripts/nv_repackager.sh
./nv_tools/scripts/nv_repackager.sh <span class="nt">-o</span> ./nv_tegra/l4t_tar_packages <span class="nt">--convert-all</span>
<span class="nb">popd</span></code></pre></figure>

<p>From the generated tarballs you only need these:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">nvidia-l4t-3d-core_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-camera_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-core_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-cuda_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-firmware_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-gbm_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-multimedia-utils_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-multimedia_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-nvfancontrol_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-nvml_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-nvpmodel_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-nvsci_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-pva_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-tools_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-vulkan-sc-sdk_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-wayland_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-x11_36.4.1-20241119120551_arm64.tbz2
nvidia-l4t-nvml_36.4.1-20241119120551_arm64.tbz2</code></pre></figure>

<p>And from this tarball <code class="language-plaintext highlighter-rouge">nvidia-l4t-init_36.4.1-20241119120551_arm64.tbz2</code> you only need these files:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">etc/asound.conf.tegra-ape
etc/asound.conf.tegra-hda-jetson-agx
etc/asound.conf.tegra-hda-jetson-xnx
etc/nvidia-container-runtime/host-files-for-container.d/devices.csv
etc/nvidia-container-runtime/host-files-for-container.d/drivers.csv
etc/nvsciipc.cfg
etc/sysctl.d/60-nvsciipc.conf
etc/systemd/nv_nvsciipc_init.sh
etc/systemd/nvpower.sh
etc/systemd/nv.sh
etc/systemd/system.conf.d/watchdog.conf
etc/systemd/system/multi-user.target.wants/nv_nvsciipc_init.service
etc/systemd/system/multi-user.target.wants/nvpower.service
etc/systemd/system/multi-user.target.wants/nv.service
etc/systemd/system/nv_nvsciipc_init.service
etc/systemd/system/nvpower.service
etc/systemd/system/nv.service
etc/udev/rules.d/99-tegra-devices.rules
usr/share/alsa/cards/tegra-ape.conf
usr/share/alsa/cards/tegra-hda.conf
usr/share/alsa/init/postinit/00-tegra.conf
usr/share/alsa/init/postinit/01-tegra-rt565x.conf
usr/share/alsa/init/postinit/02-tegra-rt5640.conf</code></pre></figure>

<p>So first let’s repackage <code class="language-plaintext highlighter-rouge">nvidia-l4t-init_36.4.1-20241119120551_arm64.tbz2</code>:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">pushd </span>Linux_for_Tegra/nv_tegra/l4t_tar_packages/
<span class="nb">cat</span> <span class="o">&gt;</span> nvidia-l4t-init.txt <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
etc/asound.conf.tegra-ape
etc/asound.conf.tegra-hda-jetson-agx
etc/asound.conf.tegra-hda-jetson-xnx
etc/nvidia-container-runtime/host-files-for-container.d/devices.csv
etc/nvidia-container-runtime/host-files-for-container.d/drivers.csv
etc/nvsciipc.cfg
etc/sysctl.d/60-nvsciipc.conf
etc/systemd/nv_nvsciipc_init.sh
etc/systemd/nvpower.sh
etc/systemd/nv.sh
etc/systemd/system.conf.d/watchdog.conf
etc/systemd/system/multi-user.target.wants/nv_nvsciipc_init.service
etc/systemd/system/multi-user.target.wants/nvpower.service
etc/systemd/system/multi-user.target.wants/nv.service
etc/systemd/system/nv_nvsciipc_init.service
etc/systemd/system/nvpower.service
etc/systemd/system/nv.service
etc/udev/rules.d/99-tegra-devices.rules
usr/share/alsa/cards/tegra-ape.conf
usr/share/alsa/cards/tegra-hda.conf
usr/share/alsa/init/postinit/00-tegra.conf
usr/share/alsa/init/postinit/01-tegra-rt565x.conf
usr/share/alsa/init/postinit/02-tegra-rt5640.conf
</span><span class="no">EOF
</span><span class="nb">tar </span>xf nvidia-l4t-init_36.4.1-20241119120551_arm64.tbz2
<span class="nb">rm </span>nvidia-l4t-init_36.4.1-20241119120551_arm64.tbz2
<span class="nb">tar </span>cjf nvidia-l4t-init_36.4.1-20241119120551_arm64.tbz2 <span class="si">$(</span><span class="nb">cat </span>nvidia-l4t-init.txt<span class="si">)</span>
<span class="nb">popd</span></code></pre></figure>

<p>On NVIDIA IGX Orin with dedicated graphics card (dGPU systems) you need to get rid of some files due to conflicts with dGPU userspace drivers.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># repackage nvidia-l4t-x11_ package</span>
<span class="nb">tar </span>tf nvidia-l4t-x11_36.4.1-20241119120551_arm64.tbz2 | <span class="nb">grep</span> <span class="nt">-v</span> /usr/bin/nvidia-xconfig <span class="se">\</span>
  <span class="o">&gt;</span> nvidia-l4t-x11_36.4.1-20241119120551.txt
<span class="nb">tar </span>xf  nvidia-l4t-x11_36.4.1-20241119120551_arm64.tbz2
<span class="nb">rm      </span>nvidia-l4t-x11_36.4.1-20241119120551_arm64.tbz2
<span class="nb">tar </span>cjf nvidia-l4t-x11_36.4.1-20241119120551_arm64.tbz2 <span class="si">$(</span><span class="nb">cat </span>nvidia-l4t-x11_36.4.1-20241119120551.txt<span class="si">)</span>

<span class="c"># repackage nvidia-l4t-3d-core_ package</span>
<span class="nb">tar </span>tf nvidia-l4t-3d-core_36.4.1-20241119120551_arm64.tbz2 | <span class="se">\</span>
  <span class="nb">grep</span> <span class="nt">-v</span> <span class="se">\</span>
       <span class="nt">-e</span> /etc/vulkan/icd.d/nvidia_icd.json <span class="se">\</span>
       <span class="nt">-e</span> /usr/lib/xorg/modules/drivers/nvidia_drv.so <span class="se">\</span>
       <span class="nt">-e</span> /usr/lib/xorg/modules/extensions/libglxserver_nvidia.so <span class="se">\</span>
       <span class="nt">-e</span> /usr/share/glvnd/egl_vendor.d/10_nvidia.json <span class="se">\</span>
       <span class="o">&gt;</span> nvidia-l4t-3d-core_36.4.1-20241119120551.txt
<span class="nb">tar </span>xf  nvidia-l4t-3d-core_36.4.1-20241119120551_arm64.tbz2
<span class="nb">rm      </span>nvidia-l4t-3d-core_36.4.1-20241119120551_arm64.tbz2
<span class="nb">tar </span>cjf nvidia-l4t-3d-core_36.4.1-20241119120551_arm64.tbz2 <span class="si">$(</span><span class="nb">cat </span>nvidia-l4t-3d-core_36.4.1-20241119120551.txt<span class="si">)</span></code></pre></figure>

<p>Then extract the generated tarballs to your system.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">pushd </span>Linux_for_Tegra/nv_tegra/l4t_tar_packages
<span class="k">for </span>i <span class="k">in</span> <span class="se">\</span>
nvidia-l4t-core_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-3d-core_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-cuda_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-firmware_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-gbm_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-multimedia-utils_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-multimedia_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-nvfancontrol_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-nvpmodel_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-tools_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-x11_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-nvsci_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-pva_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-wayland_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-camera_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-vulkan-sc-sdk_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-nvml_36.4.1-20241119120551_arm64.tbz2 <span class="se">\</span>
nvidia-l4t-init_36.4.1-20241119120551_arm64.tbz2<span class="p">;</span> <span class="k">do
  </span><span class="nb">sudo tar </span>xjf <span class="nv">$i</span> <span class="nt">-C</span> /
<span class="k">done
</span><span class="nb">popd</span></code></pre></figure>

<p>On systems without dedicated graphics (internal GPU systems) card you still
need to move</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">/usr/lib/xorg/modules/drivers/nvidia_drv.so
/usr/lib/xorg/modules/extensions/libglxserver_nvidia.so</code></pre></figure>

<p>to</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">/usr/lib64/xorg/modules/drivers/nvidia_drv.so
/usr/lib64/xorg/modules/extensions/libglxserver_nvidia.so</code></pre></figure>

<p>So let’s do this.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo mv</span> /usr/lib/xorg/modules/drivers/nvidia_drv.so <span class="se">\</span>
          /usr/lib64/xorg/modules/drivers/
<span class="nb">sudo mv</span> /usr/lib/xorg/modules/extensions/libglxserver_nvidia.so <span class="se">\</span>
          /usr/lib64/xorg/modules/extensions/
<span class="nb">sudo rm</span> <span class="nt">-rf</span> /usr/lib/xorg</code></pre></figure>

<p>Then add <code class="language-plaintext highlighter-rouge">/usr/lib/aarch64-linux-gnu</code> and
<code class="language-plaintext highlighter-rouge">/usr/lib/aarch64-linux-gnu/tegra-egl</code> to
<code class="language-plaintext highlighter-rouge">/etc/ld.so.conf.d/nvidia-tegra.conf</code>.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">echo</span> /usr/lib/aarch64-linux-gnu | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/ld.so.conf.d/nvidia-tegra.conf
<span class="nb">echo</span> /usr/lib/aarch64-linux-gnu/tegra-egl | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/ld.so.conf.d/nvidia-tegra.conf</code></pre></figure>

<p>Run ldconfig </p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>ldconfig</code></pre></figure>

<h4 id="video-group-for-regular-users">Video group for regular users</h4>

<p>A regular user needs to be added to the group <code class="language-plaintext highlighter-rouge">video</code> to be able to log in to the GNOME desktop as regular user. This can be achieved by using YaST, usermod or editing <code class="language-plaintext highlighter-rouge">/etc/group</code> manually.</p>

<h4 id="reboot-the-machine-with-the-previously-updated-kernel">Reboot the machine with the previously updated kernel</h4>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>reboot</code></pre></figure>

<p>Select first entry <code class="language-plaintext highlighter-rouge">SLES 15-SP6</code> for booting.</p>

<h3 id="basic-testing">Basic testing</h3>

<p>First basic testing will be running <code class="language-plaintext highlighter-rouge">nvidia-smi</code>. </p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>nvidia-smi</code></pre></figure>

<p>Graphical desktop (GNOME) should work as well. Linux console will also be available. Use either a serial console or a ssh connection if you don’t want to use the graphical desktop/Linux console or need remote access to the system.</p>

<h3 id="glmark2">glmark2</h3>

<p>Install phoronix-test-suite</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>zypper ar https://cdn.opensuse.org/distribution/leap/15.6/repo/oss/ repo-oss
<span class="nb">sudo </span>zypper ref
<span class="nb">sudo </span>zypper <span class="k">in </span>phoronix-test-suite</code></pre></figure>

<p>Run phoronix-test-suite</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>zypper <span class="k">in </span>gcc gcc-c++
<span class="c"># Prepare for realistic numbers</span>
<span class="c"># 1. Logout from your GNOME session</span>
<span class="c"># 2. Login again, but select IceWM Session as desktop instead of GNOME</span>
<span class="c"># 3. Start xterm and run the following command</span>
phoronix-test-suite benchmark glmark2</code></pre></figure>

<p>This should give you an <code class="language-plaintext highlighter-rouge">average score</code> of about <code class="language-plaintext highlighter-rouge">4500</code> running in <code class="language-plaintext highlighter-rouge">1920x1080</code> resolution with <code class="language-plaintext highlighter-rouge">MaxN Power</code> and best performance settings (see <code class="language-plaintext highlighter-rouge">Misc/Performance</code> and <code class="language-plaintext highlighter-rouge">Misc/MaxN/MaxN_Super Power</code> below) on <code class="language-plaintext highlighter-rouge">Jetson AGX Orin</code> and about <code class="language-plaintext highlighter-rouge">2500</code> on <code class="language-plaintext highlighter-rouge">Jetson Orin Nano</code> (also with best performance settings).</p>

<h3 id="wayland-based-desktop">Wayland based Desktop</h3>

<p>In order to enable our <code class="language-plaintext highlighter-rouge">GNOME on Wayland</code> desktop you need to install two additional packages: <code class="language-plaintext highlighter-rouge">xwayland</code> and <code class="language-plaintext highlighter-rouge">gnome-session-wayland</code>.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>zypper <span class="k">in </span>xwayland gnome-session-wayland</code></pre></figure>

<p>Afterwards restart GDM</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>systemctl restart display-manager.service</code></pre></figure>

<p>or reboot your machine.</p>

<h3 id="cudatensorflow">CUDA/Tensorflow</h3>

<h4 id="containers">Containers</h4>

<p>NVIDIA provides containers available for Jetson that include SDKs such as CUDA. More details <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-jetpack">here</a>. These containers are Ubuntu based, but can be used from SLE as well. You need to install the NVIDIA container runtime for this. Detailed information <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html">here</a>.</p>

<h5 id="1-install-podman-and-nvidia-container-runtime">1. Install podman and nvidia-container-runtime</h5>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>zypper <span class="nb">install </span>podman
<span class="nb">sudo </span>zypper ar https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo
<span class="nb">sudo </span>zypper modifyrepo <span class="nt">--enable</span> nvidia-container-toolkit-experimental
<span class="nb">sudo </span>zypper <span class="nt">--gpg-auto-import-keys</span> <span class="nb">install</span> <span class="nt">-y</span> nvidia-container-toolkit
<span class="nb">sudo </span>nvidia-ctk cdi generate <span class="nt">--mode</span><span class="o">=</span>csv <span class="nt">--output</span><span class="o">=</span>/var/run/cdi/nvidia.yaml
<span class="nb">sudo </span>nvidia-ctk cdi list</code></pre></figure>

<h5 id="2-download-the-cuda-samples">2. Download the CUDA samples</h5>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>zypper <span class="nb">install </span>git
<span class="nb">cd
</span>git clone https://github.com/NVIDIA/cuda-samples.git
<span class="nb">cd </span>cuda-samples
git checkout v12.4</code></pre></figure>

<h5 id="3-start-x">3. Start X</h5>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>rcxdm stop
<span class="nb">sudo </span>Xorg <span class="nt">-retro</span> &amp;&gt; /tmp/log &amp;
<span class="nb">export </span><span class="nv">DISPLAY</span><span class="o">=</span>:0
xterm &amp;</code></pre></figure>

<p>Monitor should now show a Moiree pattern with an unframed xterm on it. Otherwise check /tmp/log.</p>

<h5 id="4-download-and-run-the-jetpack6-container">4. Download and run the JetPack6 container</h5>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>podman run <span class="nt">--rm</span> <span class="nt">-it</span> <span class="nt">-e</span> DISPLAY <span class="nt">--net</span><span class="o">=</span>host <span class="nt">--device</span> nvidia.com/gpu<span class="o">=</span>all <span class="nt">--group-add</span> keep-groups <span class="nt">--security-opt</span> <span class="nv">label</span><span class="o">=</span>disable <span class="nt">-v</span> <span class="nv">$HOME</span>/cuda-samples:/cuda-samples nvcr.io/nvidia/l4t-jetpack:r36.2.0 /bin/bash</code></pre></figure>

<h4 id="cuda">CUDA</h4>

<h5 id="5-build-and-run-the-samples-in-the-container">5. Build and run the samples in the container</h5>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /cuda-samples
make <span class="nt">-j</span><span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span>
<span class="nb">pushd</span> ./Samples/5_Domain_Specific/nbody
make
<span class="nb">popd</span>
./bin/aarch64/linux/release/deviceQuery
./bin/aarch64/linux/release/nbody</code></pre></figure>

<h4 id="tensorrt">Tensorrt</h4>
<h5 id="6-build-and-run-tensorrt-in-the-container">6. Build and run Tensorrt in the container</h5>

<p>This is both with the GPU and DLA (deep-learning accelerator).</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">cd</span> /usr/src/tensorrt/samples/
make <span class="nt">-j</span><span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span>
<span class="nb">cd</span> ..
./bin/sample_algorithm_selector
./bin/sample_onnx_mnist
<span class="c"># Fails on Jetson Orin Nano due to lacking Deep Learning Accelerator (DLA)</span>
./bin/sample_onnx_mnist <span class="nt">--useDLACore</span><span class="o">=</span>0
./bin/sample_onnx_mnist <span class="nt">--useDLACore</span><span class="o">=</span>1</code></pre></figure>

<h3 id="misc">Misc</h3>

<h4 id="performance">Performance</h4>

<p>You can improve the performance by giving the clock a boost. For best performance you can run <code class="language-plaintext highlighter-rouge">jetson_clocks</code> to set the device to max clock settings</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>jetson_clocks <span class="nt">--show</span>
<span class="nb">sudo </span>jetson_clocks
<span class="nb">sudo </span>jetson_clocks <span class="nt">--show</span></code></pre></figure>

<p>The 1st and 3rd command just prints the clock settings.</p>

<h4 id="maxnmaxn_super-power">MaxN/MaxN_Super Power</h4>

<p>For maximum performance you also need to set <code class="language-plaintext highlighter-rouge">MaxN/MaxN_Super</code> Power. This can be done by running</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># Jetson AGX Orin</span>
<span class="nb">sudo </span>nvpmodel <span class="nt">-m</span> 0
<span class="c"># Jetson Orin Nano</span>
<span class="nb">sudo </span>nvpmodel <span class="nt">-m</span> 2</code></pre></figure>

<p>Afterwards on <code class="language-plaintext highlighter-rouge">Jetson AGX Orin</code> you need to reboot the system though.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>reboot</code></pre></figure>

<p>In order to check for the current value run</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="nb">sudo </span>nvpmodel <span class="nt">-q</span></code></pre></figure>


  </div><a class="u-url" href="/nvidia/2024/05/07/nvidia-jetson.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Stefan&#39;s openSUSE Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Stefan&#39;s openSUSE Blog</li><li><a class="u-email" href="mailto:sndirsch@suse.com">sndirsch@suse.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/sndirsch"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">sndirsch</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
